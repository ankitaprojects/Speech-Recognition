{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\n\npd.options.display.max_colwidth = 200\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames[:2]:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install librosa\n!pip install SpeechRecognition\n!pip install googletrans\n!pip install gTTs\n!pip install PyAudio\n\n!pip install googletrans==3.1.0a0\n!pip install googletrans==4.0.0-rc1","metadata":{"id":"i0rhXCu9sRag","outputId":"c3cd1396-6ab9-4dfd-a2d3-3ef3d13ed395","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import librosa\nimport IPython.display as ipd\nimport speech_recognition as sr\nfrom googletrans import Translator\nfrom gtts import gTTS","metadata":{"id":"cc1JKvVLsRas","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <center> Speech to Text ","metadata":{}},{"cell_type":"markdown","source":"## Speech Recognition Functions\n\nwe will see our recognizer in action but before we get it to work let’s see some cool functions of this instance. Speech Recognition has a built-in function to make it work with many of the APIs out there:\n    \n* recognize_bing()\n* recognize_google()\n* recognize_google_cloud()\n* recognize_wit()\n\nBing Recognizer function uses Microsoft’s cognitive services. <br>\n\nGoogle Recognizer function uses Google’s free web search API. <br>\n\nGoogle Cloud Recognizer function uses Google’s cloud speech API. <br>\n\nWit Recognizer function uses the wit.ai platform. <br>\n\nWe will use the Google Recognizer function, which is __recognize_google()__. It’s free and doesn’t require an API key to use. There is one drawback about this recognizer, it limits you when you want to work with longer audio files. In my experience, I didn’t have any issues when working with audio files under 5 minutes. I don’t recommend using this recognizer with long audio files. There are different techniques to work with longer audio files.","metadata":{}},{"cell_type":"code","source":"ipd.Audio('/kaggle/input/nlp-specialization-data/harvard.wav')","metadata":{"id":"GD99XOKxsRav","outputId":"0062b485-c9c1-41ef-a638-5bbdaa590455","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ipd.Audio('/kaggle/input/nlp-specialization-data/testSpeech.wav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ipd.Audio('/kaggle/input/nlp-specialization-data/singleEnglishWord.wav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r = sr.Recognizer()","metadata":{"id":"jGlBZz-0sRat","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"harvard = sr.AudioFile('/kaggle/input/nlp-specialization-data/testSpeech.wav')\nwith harvard as source:\n    audio = r.record(source)","metadata":{"id":"ROipCgCPsRau","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(audio)","metadata":{"id":"5ubiANJesRa0","outputId":"6b8480b2-6e15-4000-e057-1292a0192f45","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text=r.recognize_google(audio)\ntext","metadata":{"id":"XukZ_ZL1sRa1","outputId":"45939f5a-2bb5-42cf-82d9-7a4b0f7a8ec6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note:**\n\n* The __offset__ and __duration__ keyword arguments are useful for segmenting an audio file if you have prior knowledge of the structure of the speech in the file.\n\n* Offset For instance, if you do not want to transcribe the first 4 seconds of the audio, pass 4 as the value for the offset attribute. As an example, the following script skips the first 4 seconds of the audio file","metadata":{"id":"EO88vd1_sRa1"}},{"cell_type":"code","source":"with harvard as source:\n    audio = r.record(source, offset=4)\nr.recognize_google(audio)","metadata":{"id":"9qg2dQn9sRa2","outputId":"f7f6c869-d72b-47ac-c99a-d33ae661ba89","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with harvard as source:\n    audio = r.record(source, duration=3)\nr.recognize_google(audio)","metadata":{"id":"hkGwybkQsRa3","outputId":"abc905fe-9e28-4dc3-dd02-ff9b55f9ce45","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with harvard as source:\n    audio = r.record(source, offset=4, duration=5)\nr.recognize_google(audio)","metadata":{"id":"e5hpfZ3_sRa3","outputId":"866c348d-25bc-460a-c11e-fde2c281f221","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <center> Speech to Speech","metadata":{"id":"R0AYwW4hsRa3"}},{"cell_type":"code","source":"#!pip install google_trans_new\n\n# from google_trans_new import google_translator  \n# translator = google_translator()  \n# translate_text = translator.translate(\"hello\", lang_src='en',lang_tgt='hi')  \n# print(translate_text)\n# #output: Hello china","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r = sr.Recognizer()\nharvard = sr.AudioFile('/kaggle/input/nlp-specialization-data/harvard.wav')\nwith harvard as source:\n    audio = r.record(source)\ntext=r.recognize_google(audio)\ntext","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"translatorInstance=Translator()\n?translatorInstance.translate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"translatedText = translatorInstance.translate(text=text,dest='hi',src='en')\nprint(translatedText.text.split(' '))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mytext = translator.translate(text,dest='la').text\nlanguage = 'hi'\ntranslatedObject= gTTS(text=translatedText.text, lang=language, slow=False)\ntranslatedObject.save('translatedAudio.mp3') ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import IPython.display as ipd\nipd.Audio('translatedAudio.mp3')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"translatedText = translatorInstance.translate(text,dest='ur').text.split(' ') \nprint(translatedText)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"translatedText = translatorInstance.translate(text,dest='es').text.split(' ') \nprint(translatedText)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Try Yourself Now! ","metadata":{}},{"cell_type":"code","source":"# !pip install -U pip\n# !pip install -U cython\n!apt-get install portaudio19-dev\n!pip install PyAudio","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def recognize_speech_from_mic(recognizer, microphone):\n    \"\"\"Transcribe speech from recorded from `microphone`.\n    Returns a dictionary with three keys:\n    \"success\": a boolean indicating whether or not the API request was\n               successful\n    \"error\":   `None` if no error occured, otherwise a string containing\n               an error message if the API could not be reached or\n               speech was unrecognizable\n    \"transcription\": `None` if speech could not be transcribed,\n               otherwise a string containing the transcribed text\n    \"\"\"\n    # check that recognizer and microphone arguments are appropriate type\n    if not isinstance(recognizer, sr.Recognizer):\n        raise TypeError(\"`recognizer` must be `Recognizer` instance\")\n\n    if not isinstance(microphone, sr.Microphone):\n        raise TypeError(\"`microphone` must be `Microphone` instance\")\n\n    # adjust the recognizer sensitivity to ambient noise and record audio\n    # from the microphone\n    with microphone as source:\n        recognizer.adjust_for_ambient_noise(source) # #  analyze the audio source for 1 second\n        audio = recognizer.listen(source)\n\n    # set up the response object\n    response = {\n        \"success\": True,\n        \"error\": None,\n        \"transcription\": None\n    }\n\n    # try recognizing the speech in the recording\n    # if a RequestError or UnknownValueError exception is caught,\n    #   update the response object accordingly\n    try:\n        response[\"transcription\"] = recognizer.recognize_google(audio)\n    except sr.RequestError:\n        # API was unreachable or unresponsive\n        response[\"success\"] = False\n        response[\"error\"] = \"API unavailable/unresponsive\"\n    except sr.UnknownValueError:\n        # speech was unintelligible\n        response[\"error\"] = \"Unable to recognize speech\"\n\n    return response","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    recognizer = sr.Recognizer()\n    mic = sr.Microphone(device_index=1)\n    response = recognize_speech_from_mic(recognizer, mic)\n    print('\\nSuccess : {}\\nError   : {}\\n\\nText from Speech\\n{}\\n\\n{}' \\\n          .format(response['success'],\n                  response['error'],\n                  '-'*17,\n                  response['transcription']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <center> Bonus","metadata":{}},{"cell_type":"markdown","source":"### 1. Speechless Audio","metadata":{}},{"cell_type":"code","source":"# Import the silent audio file\nsilent_audio_file = sr.AudioFile(\"silent_audio.wav\")\n# Convert the AudioFile to AudioData\nwith silent_audio_file as source:\n    silent_audio = recognizer.record(source)\n# Recognize the AudioData with show_all turned on\nrecognizer.recognize_google(silent_audio, show_all=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Multiple Speakers\nThis process of understanding the different speakers in a single audio file is known as speaker diarization. This is a really cool function to have but unfortunately, it is not available in this library. One solution to do this is to have different audio files for different speakers, go through them using for loop.","metadata":{}},{"cell_type":"code","source":"recognizer = sr.Recognizer()\n# Multiple speakers on different files\nspeakers = [sr.AudioFile(\"speaker_0.wav\"), sr.AudioFile(\"speaker_1.wav\"), sr.AudioFile(\"speaker_2.wav\")]\n# Transcribe each speaker individually\nfor i, speaker in enumerate(speakers):\n    with speaker as source:\n    speaker_audio = recognizer.record(source)\nprint(f\"Text from speaker {i}:\")\nprint(recognizer.recognize_google(speaker_audio,language=\"en-US\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Background Noise\nTo handle the background noise, the recognizer class has a built-in function called adjust_for_ambient_noise function, which also takes a parameter of duration. Using this function the recognizer class listens to the audio for the specified duration seconds from the beginning of the audio and then adjusts the energy threshold value so that the whole audio is more recognizable.","metadata":{}},{"cell_type":"code","source":"# Import audio file with background nosie\nnoisy_audio_file = sr.AudioFile(\"noisy_audio.wav\")\n# Adjust for ambient noise and record\nwith noisy_audio_file as source:\n    recognizer.adjust_for_ambient_noise(source, duration=0.5)\n    noisy_audio_file = recognizer.record(source)\n# Recognize the audio\nrecognizer.recognize_google(noisy_audio_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reference Links \n\n* __The Ultimate Guide To Speech Recognition With Python -__ https://realpython.com/python-speech-recognition/\n\n* __Learn how to Build your own Speech-to-Text Model (using Python) -__ www.analyticsvidhya.com/blog/2019/07/learn-build-first-speech-to-text-model-python/","metadata":{}}]}